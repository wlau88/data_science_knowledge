{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Implementing Gradient Descent and exploring its effects on regularized and non-regularized functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gradient_ascent import GradientAscent\n",
    "from regression_functions import log_likelihood, log_likelihood_gradient, \\\n",
    "                                 predict, accuracy, precision, recall\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data/testdata.csv', delimiter=',')\n",
    "X = data[:,0:2]\n",
    "y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_pos = X[:,0][y>0]\n",
    "x2_pos = X[:,1][y>0]\n",
    "x1_neg = X[:,0][y == 0]\n",
    "x2_neg = X[:,1][y == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x109e05890>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEPCAYAAACqZsSmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109bac7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1_pos,x2_pos, color = 'r', label = 'Positive')\n",
    "plt.scatter(x1_neg,x2_neg, color = 'b', label = 'Negative')\n",
    "plt.xlabel('First Feature')\n",
    "plt.ylabel('Second Feature')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Implemting cost function and gradient ascent. Testing with a simple example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is the *log likelihood*. Our goal will be to *maximize* this value, so we will actually be implementing gradient *ascent*.\n",
    "\n",
    "![Cost Function](images/logit_cost_function.gif)\n",
    "\n",
    "Recall that the hypothesis function *h* is defined as follows:\n",
    "\n",
    "![hypothesis](images/logit_sigmoid.gif)\n",
    "\n",
    "You will also need to have a function which calculates the gradient of the cost function, which is as follows.\n",
    "\n",
    "![gradient](images/logit_gradient.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.3314116154360329"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 1], [2, 2]])\n",
    "y = np.array([1, 0])\n",
    "coeffs = np.array([1, 1])\n",
    "log_likelihood(X, y, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Gradient Ascent implementation:\n",
      "  coeffs: [ 2.8876749  -4.38078995  0.69727993]\n",
      "  accuracy: 1.0\n",
      "  precision: 1.0\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient Ascent\n",
    "ga = GradientAscent(log_likelihood, log_likelihood_gradient, predict)\n",
    "ga.run(X, y, step_size=.0001)\n",
    "print \"Results from Gradient Ascent implementation:\"\n",
    "print \"  coeffs:\", ga.coeffs\n",
    "y_pred = ga.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Stochastic Gradient Ascent implementation:\n",
      "  coeffs: [ 2.88771268 -4.38088637  0.69726949]\n",
      "  accuracy: 1.0\n",
      "  precision: 1.0\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Ascent\n",
    "sga = GradientAscent(log_likelihood, log_likelihood_gradient, predict)\n",
    "sga.sgd_run(X, y)\n",
    "print \"Results from Stochastic Gradient Ascent implementation:\"\n",
    "print \"  coeffs:\", sga.coeffs\n",
    "y_pred = sga.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from sklearn Logistic Regression:\n",
      "  coeffs: 0.199560772895 -0.535180361599 -0.0680294079043\n"
     ]
    }
   ],
   "source": [
    "# sklearn's Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "print \"Results from sklearn Logistic Regression:\"\n",
    "print \"  coeffs:\", lr.intercept_[0], lr.coef_[0][0], lr.coef_[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Gradient Ascent with regularization\n",
      "  coeffs: [ 0.08565445 -0.32670159 -0.07769635]\n",
      "  accuracy: 1.0\n",
      "  precision: 1.0\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient Ascent with regularization\n",
    "likelihood_regularized = lambda X, y, coeffs: \\\n",
    "                         log_likelihood(X, y, coeffs, l=10)\n",
    "gradient_regularized = lambda X, y, coeffs: \\\n",
    "                       log_likelihood_gradient(X, y, coeffs, l=1)\n",
    "gar = GradientAscent(likelihood_regularized, gradient_regularized, predict)\n",
    "gar.run(X, y)\n",
    "print \"Results from Gradient Ascent with regularization\"\n",
    "print \"  coeffs:\", gar.coeffs\n",
    "y_pred = gar.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Since there are many different dividing plane. The results are quite different from sklearn's implementation. Regularized is the closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing with original testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data/testdata.csv', delimiter=',')\n",
    "X = data[:,0:2]\n",
    "y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3417114164204169"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(X, y, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Gradient Ascent implementation:\n",
      "  coeffs: [ 0.03835756  1.59713955  0.01972872]\n",
      "  accuracy: 1.0\n",
      "  precision: 1.0\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient Ascent\n",
    "ga = GradientAscent(log_likelihood, log_likelihood_gradient, predict)\n",
    "ga.run(X, y, step_size=.0001)\n",
    "print \"Results from Gradient Ascent implementation:\"\n",
    "print \"  coeffs:\", ga.coeffs\n",
    "y_pred = ga.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Stochastic Gradient Ascent implementation:\n",
      "  coeffs: [ 0.03836344  1.5971283   0.01975378]\n",
      "  accuracy: 1.0\n",
      "  precision: 1.0\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Ascent\n",
    "sga = GradientAscent(log_likelihood, log_likelihood_gradient, predict)\n",
    "sga.sgd_run(X, y)\n",
    "print \"Results from Stochastic Gradient Ascent implementation:\"\n",
    "print \"  coeffs:\", sga.coeffs\n",
    "y_pred = sga.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from sklearn Logistic Regression:\n",
      "  coeffs: 0.0465275107936 1.23185610298 0.0225170865919\n"
     ]
    }
   ],
   "source": [
    "# sklearn's Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "print \"Results from sklearn Logistic Regression:\"\n",
    "print \"  coeffs:\", lr.intercept_[0], lr.coef_[0][0], lr.coef_[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Gradient Ascent with regularization\n",
      "  coeffs: [ 0.03035581  1.10588056  0.01536472]\n",
      "  accuracy: 1.0\n",
      "  precision: 1.0\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient Ascent with regularization\n",
    "likelihood_regularized = lambda X, y, coeffs: \\\n",
    "                         log_likelihood(X, y, coeffs, l=10)\n",
    "gradient_regularized = lambda X, y, coeffs: \\\n",
    "                       log_likelihood_gradient(X, y, coeffs, l=1)\n",
    "gar = GradientAscent(likelihood_regularized, gradient_regularized, predict)\n",
    "gar.run(X, y)\n",
    "print \"Results from Gradient Ascent with regularization\"\n",
    "print \"  coeffs:\", gar.coeffs\n",
    "y_pred = gar.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Since there are again many different dividing plane. The results are quite different from sklearn's implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing On Grad School Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_df = pd.read_csv('data/grad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = grad_df.admit.values\n",
    "X = grad_df.drop('admit', axis=1).values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Gradient Ascent implementation:\n",
      "  coeffs: [-2.68792228  1.40987806  1.78138334 -1.40600439]\n",
      "  accuracy: 0.705\n",
      "  precision: 0.6\n",
      "  recall: 0.212598425197\n"
     ]
    }
   ],
   "source": [
    "# Gradient Ascent\n",
    "ga = GradientAscent(log_likelihood, log_likelihood_gradient, predict, scale=True)\n",
    "ga.run(X, y, num_iterations=100000)\n",
    "print \"Results from Gradient Ascent implementation:\"\n",
    "print \"  coeffs:\", ga.coeffs\n",
    "y_pred = ga.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /Users/Warren/GitHub/data_science_knowledge/gradient_descent/gradient_ascent.py(76)sgd_run()\n",
      "-> self.coeffs = np.zeros(X.shape[1])\n",
      "(Pdb) c\n",
      "Results from Stochastic Gradient Ascent implementation:\n",
      "  coeffs: [-0.04522513  0.00772274 -0.06986325 -0.38252642]\n",
      "  accuracy: 0.3175\n",
      "  precision: 0.3175\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Ascent\n",
    "sga = GradientAscent(log_likelihood, log_likelihood_gradient, predict)\n",
    "sga.sgd_run(X, y, step_size=.001)\n",
    "print \"Results from Stochastic Gradient Ascent implementation:\"\n",
    "print \"  coeffs:\", sga.coeffs\n",
    "y_pred = sga.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from sklearn Logistic Regression:\n",
      "  coeffs: -1.18847875568 [ 0.00191577  0.21564289 -0.59842009]\n",
      "  accuracy: 0.715\n",
      "  precision: 0.651162790698\n",
      "  recall: 0.220472440945\n"
     ]
    }
   ],
   "source": [
    "# sklearn's Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "print \"Results from sklearn Logistic Regression:\"\n",
    "print \"  coeffs:\", lr.intercept_[0], lr.coef_[0]\n",
    "y_pred = lr.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from Gradient Ascent with regularization\n",
      "  coeffs: [-0.97053865  0.76129108  0.40038977 -1.27163495]\n",
      "  accuracy: 0.705\n",
      "  precision: 0.645161290323\n",
      "  recall: 0.157480314961\n"
     ]
    }
   ],
   "source": [
    "# Gradient Ascent with regularization\n",
    "likelihood_regularized = lambda X, y, coeffs: \\\n",
    "                         log_likelihood(X, y, coeffs, l=10)\n",
    "gradient_regularized = lambda X, y, coeffs: \\\n",
    "                       log_likelihood_gradient(X, y, coeffs, l=1)\n",
    "gar = GradientAscent(likelihood_regularized, gradient_regularized, predict, scale=True)\n",
    "gar.run(X, y, num_iterations=100000)\n",
    "print \"Results from Gradient Ascent with regularization\"\n",
    "print \"  coeffs:\", gar.coeffs\n",
    "y_pred = gar.predict(X)\n",
    "print \"  accuracy:\", accuracy(y, y_pred)\n",
    "print \"  precision:\", precision(y, y_pred)\n",
    "print \"  recall:\", recall(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####SGD was not able to converge. Gradient Ascent and Gradient Ascent with regularization did best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton(f,f_p,f_pp,tol):\n",
    "    error = tol+1\n",
    "    x_0 =1\n",
    "    while error > tol:\n",
    "        print x_0\n",
    "        x_0 -= f_p(x_0)/float(f_pp(x_0))\n",
    "        error = f_p(x_0)\n",
    "    return x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 6*x**2+3*x-10\n",
    "def f_p(x):\n",
    "    return 12*x + 3\n",
    "def f_pp(x):\n",
    "    return 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.25"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(f,f_p,f_pp, tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
